{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qMIsk3ptips1"
      },
      "source": [
        "## imports and installations"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MVC220rl98Dw"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V6n03oFvZt2W",
        "outputId": "d295d84d-ffd8-44db-b161-d033433de7cf"
      },
      "outputs": [],
      "source": [
        "!pip install pyspoc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pyspoc[all]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pHNyOWvqbiXN",
        "outputId": "2d98857d-87cc-4848-e078-7ddf51ad9352"
      },
      "outputs": [],
      "source": [
        "%env PYDEVD_DISABLE_FILE_VALIDATION=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ya3YP-ecNs6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"PYDEVD_DISABLE_FILE_VALIDATION\"] = \"1\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "1TwA8xUNVlua",
        "outputId": "ab3e23b9-e97d-4af6-eaba-1966adec7302"
      },
      "outputs": [],
      "source": [
        "# install some prerequisite packages for pyspi\n",
        "!pip install oct2py\n",
        "!pip install jpype1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UI6h6HUdcexD",
        "outputId": "467a9d7b-8542-4ded-9e82-50533dd2e307"
      },
      "outputs": [],
      "source": [
        "from pyspoc import Calculator, Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPgYCB4xVVGn"
      },
      "outputs": [],
      "source": [
        "# import other helper libraries\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YPbmikbTi09q"
      },
      "source": [
        "## data generation\n",
        "\n",
        "currently just random integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5LyTebxgVZ3B"
      },
      "outputs": [],
      "source": [
        "# generate a random dataframe\n",
        "df = pd.DataFrame(np.random.randint(0,100,size=(100, 4)), columns=list('ABCD'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLcPDfU2A4Qa"
      },
      "source": [
        "## Simple Example in PySS: using **Statistics** and **Reducers**\n",
        "\n",
        "https://github.com/garry-cotton/pyss\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lak0H5IbW6g4"
      },
      "outputs": [],
      "source": [
        "# create a simple YAML config for pyss\n",
        "\n",
        "# we first configure the 'Statistics' we want to use\n",
        "# in this eg, we use the Covariance and the KendallTau correlation coefficient\n",
        "\n",
        "# we then configure the 'Reducers' we want to use\n",
        "# in this eg, we use the SingularValues and the Determinant\n",
        "\n",
        "# for each Reducer function, we determine which Statistic to apply it to reduce\n",
        "\n",
        "yaml_str_stat_reduce = \"\"\"\n",
        "Statistics:\n",
        "  pyspoc.statistics.basic:\n",
        "    # Covariance\n",
        "    Covariance:\n",
        "      schemes:\n",
        "        cov:\n",
        "          estimator: EmpiricalCovariance\n",
        "\n",
        "    # Kendall Tau correlation coefficient\n",
        "    KendallTau:\n",
        "      schemes:\n",
        "        sq:\n",
        "          squared: True\n",
        "\n",
        "        standard:\n",
        "          squared: False\n",
        "\n",
        "Reducers:\n",
        "  pyspoc.reducers.basic:\n",
        "    SingularValues:\n",
        "      Statistics:\n",
        "        - pyspoc.statistics.basic.Covariance.cov\n",
        "        - pyspoc.statistics.basic.KendallTau.sq\n",
        "        - pyspoc.statistics.basic.KendallTau.standard\n",
        "      schemes:\n",
        "        three:\n",
        "          num_values: 3\n",
        "\n",
        "    Determinant:\n",
        "      Statistics:\n",
        "        - pyspoc.statistics.basic.Covariance.cov\n",
        "      schemes:\n",
        "        scaled:\n",
        "          scaled: True\n",
        "\n",
        "        non-scaled:\n",
        "          scaled: False\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 668
        },
        "id": "iIP6WUyqVNLF",
        "outputId": "8e493edd-e9a9-4249-ab37-580337629531"
      },
      "outputs": [],
      "source": [
        "# copy the randomly generated dataframe\n",
        "my_dataset = df.copy()\n",
        "\n",
        "# set a name for the config\n",
        "my_config_name = \"test_config_yml\"\n",
        "# create a Config class object\n",
        "cfg = Config.from_yaml(my_config_name, yaml_str_stat_reduce)\n",
        "# pass the dataframe or numpy array\n",
        "# constructs a 'Calculator' instance\n",
        "calc = Calculator(my_dataset, normalise=False)\n",
        "# run the compute function - applies functions in the config (i.e. in Statistic and Reducer) to the Data\n",
        "calc.compute(cfg)\n",
        "# view results\n",
        "calc.results.T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fO4Tb-Vz3hT0"
      },
      "outputs": [],
      "source": [
        "REDUCERS = list(calc.results.columns.values)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ALfdZ3giIH9"
      },
      "source": [
        "## Simple Example in PySS: using **ReducedStatistics**\n",
        "\n",
        "https://github.com/garry-cotton/pyss\n",
        "\n",
        "to illustrate the use of the ReducedStatistics, we use two PCA-based statistics and one inherited from the Statistics class (since any Statistic can be used as a ReducedStatistic also)\n",
        "\n",
        "more Statistics, Reducers, and ReducedStatistics can be added similarly to how it's done for ReducedStatistics in ```pyss/rstatistics/test.py```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e3orEd3KiKlS"
      },
      "outputs": [],
      "source": [
        "yaml_reducedStat = \"\"\"\n",
        "ReducedStatistics:\n",
        "  pyspoc.rstatistics.pca:\n",
        "    PCAVarianceExplainedRatio:\n",
        "      schemes:\n",
        "        std:\n",
        "          components:\n",
        "            - 2\n",
        "            - 4\n",
        "\n",
        "    PCAEigenVectors:\n",
        "      schemes:\n",
        "        std:\n",
        "          principal_vectors:\n",
        "            - 1\n",
        "            - 3\n",
        "\n",
        "  pyspoc.rstatistics.test:\n",
        "    ReducedCovarianceTest:\n",
        "      schemes:\n",
        "        cov:\n",
        "          estimator: EmpiricalCovariance\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrgsyxWWjERY"
      },
      "outputs": [],
      "source": [
        "# # copy the randomly generated dataframe\n",
        "# my_dataset_rs = df.copy()\n",
        "\n",
        "# # set a name for the config\n",
        "# my_config_name_rs = \"test_config_yml_rs\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjQAYdtjjERb",
        "outputId": "e9c7981e-b85c-44de-81bd-9787ffba886e"
      },
      "outputs": [],
      "source": [
        "# # create a Config class object\n",
        "# cfg_rs = Config.from_yaml(my_config_name_rs, yaml_reducedStat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njj8aUmRjERb",
        "outputId": "7f8453b6-bc95-4189-c140-6fff745b4aab"
      },
      "outputs": [],
      "source": [
        "# # pass the dataframe or numpy array\n",
        "# # constructs a 'Calculator' instance\n",
        "# calc_rs = Calculator(my_dataset_rs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejoIyh4TjERd",
        "outputId": "32678dc6-a6f4-4a45-c38c-ad51ccd37995"
      },
      "outputs": [],
      "source": [
        "# # run the compute function - applies functions in the config (i.e. in Statistic and Reducer) to the Data\n",
        "# calc_rs.compute(cfg_rs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "3kuxSccSjERd",
        "outputId": "bebcf506-8d26-48d2-8463-ed7e3e198f5f"
      },
      "outputs": [],
      "source": [
        "# # view results\n",
        "# calc_rs.results.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rku77IlFpdmM"
      },
      "source": [
        "##Functions to Load and manipulate dataframe as appropriate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UKonyZ9spjEr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats\n",
        "from typing import List, Dict, Union, Callable, Optional, Any, Tuple\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from matplotlib.figure import Figure\n",
        "from tqdm.notebook import tqdm\n",
        "from matplotlib.backends.backend_pdf import PdfPages\n",
        "\n",
        "\n",
        "\n",
        "def load_data(df) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Drop non-numeric columns and convert to numeric.\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        The DataFrame containing the data to change.\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        The loaded data.\n",
        "    \"\"\"\n",
        "    \n",
        "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
        "    df = df[numeric_cols]  # Keep only numeric columns\n",
        "    df = df.apply(pd.to_numeric, errors='coerce')\n",
        "    df = df.dropna()\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def normalize_data(data: pd.DataFrame, method: str = 'z-score') -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Normalize the data using the specified method.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    data : pd.DataFrame\n",
        "        The data to normalize.\n",
        "    method : str, optional\n",
        "        The method to use for scaling. Options are:\n",
        "        - 'z-score': Z-Score Normalization (StandardScaler)\n",
        "        - 'min-max': Min-Max Normalization (MinMaxScaler)\n",
        "        - 'robust': Robust Scaling using median and IQR (RobustScaler)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        The normalized data.\n",
        "    \"\"\"\n",
        "    normalized = data.copy()\n",
        "\n",
        "    # Select the appropriate scaler based on the method\n",
        "    if method == 'z-score':\n",
        "        scaler = StandardScaler()\n",
        "    elif method == 'min-max':\n",
        "        scaler = MinMaxScaler()\n",
        "    elif method == 'robust':\n",
        "        scaler = RobustScaler()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown normalization method: {method}. \"\n",
        "                         f\"Choose from 'z-score', 'min-max', or 'robust'.\")\n",
        "\n",
        "    # Apply the scaler to the data\n",
        "    normalized_values = scaler.fit_transform(normalized)\n",
        "    normalized = pd.DataFrame(normalized_values,\n",
        "                             index=normalized.index,\n",
        "                             columns=normalized.columns)\n",
        "\n",
        "    return normalized\n",
        "\n",
        "\n",
        "def shuffle_data(df: pd.DataFrame, random_seed=None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Returns a new DataFrame with each column's elements shuffled independently.\n",
        "\n",
        "    Parameters:\n",
        "    - df: The input pandas DataFrame.\n",
        "    - random_seed: Optional seed for reproducibility.\n",
        "\n",
        "    Returns:\n",
        "    - A new DataFrame with columns shuffled.\n",
        "    \"\"\"\n",
        "    rng = np.random.default_rng(seed=random_seed)\n",
        "    return pd.DataFrame(\n",
        "        [rng.permutation(df[col]) for col in df.columns],\n",
        "        index=df.columns,\n",
        "        columns=df.index\n",
        "    ).T\n",
        "\n",
        "def perturb_dataframe(df, scale_factor=0.1, random_seed=None):\n",
        "    \"\"\"    \n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pandas.DataFrame\n",
        "        Input DataFrame containing numeric values\n",
        "    scale_factor : float, default=0.1\n",
        "        Scaling factor for the perturbation\n",
        "    random_seed : int, optional\n",
        "        Random seed for reproducible results\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "    pandas.DataFrame\n",
        "        DataFrame with perturbed values\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        np.random.seed(random_seed)\n",
        "    \n",
        "    # Calculate column variances\n",
        "    col_variances = df.var(axis=0, ddof=1)\n",
        "    col_variances = np.maximum(col_variances, 1e-6)  # Minimum variance\n",
        "    \n",
        "    # Calculate perturbation standard deviations\n",
        "    perturbation_stds = scale_factor * np.sqrt(col_variances)\n",
        "    \n",
        "    # Create perturbation matrix\n",
        "    perturbations = np.random.normal(0, 1, size=df.shape)\n",
        "    \n",
        "    # Scale perturbations by column-specific standard deviations\n",
        "    for j in range(df.shape[1]):\n",
        "        perturbations[:, j] *= perturbation_stds.iloc[j]\n",
        "    \n",
        "    # Add perturbations to original data\n",
        "    perturbed_df = df + perturbations\n",
        "    \n",
        "    return perturbed_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFiJr-V6qwkk"
      },
      "source": [
        "##Applying reducers to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PHobS1lb4W0v",
        "outputId": "9d53fda4-b776-4ea2-ca18-0da928f32ec5"
      },
      "outputs": [],
      "source": [
        "#defining reducers\n",
        "REDUCERS = list(calc.results.columns.values)\n",
        "for reducer in REDUCERS:\n",
        "    print(calc.results[reducer].values[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIALIT0Oq08Y"
      },
      "outputs": [],
      "source": [
        "def apply_reducer(calc, reducer) -> float:\n",
        "    \"\"\"\n",
        "    Apply a specific reducer to a matrix and get a value, useful for iterating\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    calc : Calculator\n",
        "        The calculator instance.\n",
        "    reducer : gets reducer value from calc.results\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    float\n",
        "        The reduced value.\n",
        "    \"\"\"\n",
        "    return calc.results[reducer].values[0]\n",
        "\n",
        "\n",
        "def pyspi_calc(df,config_name, yaml, normalize=None, shuffle=False, perturb=False,scale_factor=0.1,random_seed=None):\n",
        "    \"\"\"\n",
        "    Calculate covariance and other statistics from a CSV file.\n",
        "\n",
        "    Parameters:\n",
        "    - df: DataFrame containing the data to change\n",
        "    - config_name: Name of the configuration file\n",
        "    - yaml: YAML string for configuration\n",
        "    - normalize: Method to normalize data (default: None)\n",
        "    - shuffle: Whether to randomly shuffle rows (default: False)\n",
        "    - random_seed: Seed for reproducible shuffling (default: None)\n",
        "\n",
        "    Returns:\n",
        "    - Calc results table\n",
        "    \"\"\"\n",
        "\n",
        "    # Load data \n",
        "    df = load_data(df)\n",
        "\n",
        "    # If shuffle is requested, shuffle the data\n",
        "    if shuffle:\n",
        "        df = shuffle_data(df, random_seed=random_seed)\n",
        "\n",
        "    # If perturb is requested, add random noise to the data\n",
        "    if perturb:\n",
        "        df = perturb_dataframe(df, scale_factor=scale_factor, random_seed=random_seed)\n",
        "    if normalize is not None:\n",
        "        df = normalize_data(df, method=normalize)\n",
        "    cfg = Config.from_yaml(config_name, yaml)\n",
        "    # Use Calculator class to compute the statistics\n",
        "    # pass the dataframe or numpy array\n",
        "    # constructs a 'Calculator' instance\n",
        "    calc = Calculator(df, normalise=False)\n",
        "    # run the compute function - applies functions in the config (i.e. in Statistic and Reducer) to the Data\n",
        "    calc.compute(cfg)\n",
        "    return calc.results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sUFX_Hr41JMr"
      },
      "outputs": [],
      "source": [
        "def permutation_test(\n",
        "    df: pd.DataFrame,\n",
        "    config_name,\n",
        "    yaml,\n",
        "    calc,\n",
        "    n_permutations: int = 1000,\n",
        "    normalize: Optional[str] = None,\n",
        "    random_seed: Optional[int] = None,\n",
        "    save_permutation_data: bool = True,\n",
        "    orig_calc_table: Optional[Dict] = None,\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Perform a permutation test to assess statistical significance and calculate variance.\n",
        "    \"\"\"\n",
        "    REDUCERS = list(calc.columns.values)\n",
        "    # Use provided orig_calc_table or compute\n",
        "    if orig_calc_table is None:\n",
        "        orig_calc_table = pyspi_calc(df, config_name, yaml, normalize=normalize)\n",
        "\n",
        "    orig_results = {\n",
        "        reducer: orig_calc_table[reducer].values[0] for reducer in REDUCERS\n",
        "    }\n",
        "    \n",
        "    perm_values = {reducer: [] for reducer in REDUCERS}\n",
        "\n",
        "    # Run permutations with progress bar\n",
        "    iter_range = tqdm(range(n_permutations))\n",
        "    for i in iter_range:\n",
        "        # Use different random seed for each perm\n",
        "        perm_seed = None if random_seed is None else random_seed + i + 1\n",
        "        # Get shuffled data statistics\n",
        "        perm_calc_table = pyspi_calc(df, config_name,yaml, normalize=normalize, shuffle=True, random_seed=perm_seed)\n",
        "\n",
        "        for reducer in REDUCERS:\n",
        "            perm_value = perm_calc_table[reducer].values[0]\n",
        "            # Append the value to the list\n",
        "            perm_values[reducer].append(perm_value)\n",
        "\n",
        "    # Calculate p-values, variances, and add to results\n",
        "    # Create the DataFrame of permutation results AFTER collecting all values\n",
        "    # The index length should match the number of permutations\n",
        "    perm_data = pd.DataFrame(perm_values)\n",
        "    # Calculate p-values, variances, and add to results\n",
        "    final_results = {}\n",
        "    for reducer in REDUCERS:\n",
        "        # Access the specific reducer's data from the perm_data DataFrame\n",
        "        perm_data_series = perm_data[reducer]\n",
        "        orig_value = orig_results[reducer]\n",
        "        n = len(perm_data_series) # Use the length of the collected data\n",
        "\n",
        "        # Calculate permutation percentile for p-value\n",
        "        lower_tail = np.sum(perm_data_series < orig_value) / n\n",
        "        upper_tail = np.sum(perm_data_series > orig_value) / n\n",
        "        permutation_percentile = min(lower_tail, upper_tail)\n",
        "        p_value = permutation_percentile * 2 # Multiply by 2 for a two-tailed test\n",
        "\n",
        "        result = {'value': orig_value,\n",
        "              'p_value': p_value}\n",
        "\n",
        "        # Update results with permutation statistics\n",
        "        result.update({\n",
        "            'permutation_mean': np.mean(perm_data_series),\n",
        "            'permutation_median': np.median(perm_data_series),\n",
        "            'permutation_std': np.std(perm_data_series),\n",
        "            'permutation_std_err': np.std(perm_data_series) / np.sqrt(n),\n",
        "            'permutation_variance': np.var(perm_data_series),\n",
        "            'permutation_min': np.min(perm_data_series),\n",
        "            'permutation_max': np.max(perm_data_series),\n",
        "            'permutation_distribution': perm_data_series.tolist() if save_permutation_data else None, # Store as list if needed\n",
        "            'permutation_percentile_rank': permutation_percentile\n",
        "        })\n",
        "\n",
        "        final_results[reducer] = result\n",
        "    return final_results\n",
        "\n",
        "\n",
        "def plot_permutation_distribution(results: Dict, reducer: str,\n",
        "                                 figsize: Tuple[int, int] = (10, 6)) -> Figure:\n",
        "    \"\"\"\n",
        "    Plot the distribution of permutation test values\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    results : Dict\n",
        "        Dictionary containing permutation test results.\n",
        "    reducer : str\n",
        "        Name of the reducer\n",
        "    figsize : Tuple[int, int], optional\n",
        "        Figure size (width, height), by default (10, 6)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Figure\n",
        "        Matplotlib figure object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract data\n",
        "    result = results[reducer]\n",
        "    perm_values = result['permutation_distribution']\n",
        "    orig_value = result['value']\n",
        "    p_value = result['p_value']\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Plot histogram of permutation values\n",
        "    n, bins, patches = ax.hist(perm_values, bins=30, alpha=0.7, color='skyblue',\n",
        "                              edgecolor='black', density=True)\n",
        "\n",
        "    # Add kernel density estimate\n",
        "    sns.kdeplot(perm_values, color='navy', ax=ax, linewidth=2)\n",
        "\n",
        "    # Add vertical line for original value\n",
        "    ax.axvline(x=orig_value, color='red', linestyle='--', linewidth=2,\n",
        "              label=f'Original value: {orig_value:.4f}')\n",
        "\n",
        "    # Add permutation statistics\n",
        "    stats_text = (\n",
        "        f\"Permutation Stats:\\n\"\n",
        "        f\"Mean: {result['permutation_mean']:.4f}\\n\"\n",
        "        f\"Median: {result['permutation_median']:.4f}\\n\"\n",
        "        f\"Std Dev: {result['permutation_std']:.4f}\\n\"\n",
        "        f\"Std Err: {result['permutation_std_err']:.4f}\\n\"\n",
        "        f\"p-value: {p_value:.4f}\"\n",
        "    )\n",
        "\n",
        "    # Add the stats text as an annotation\n",
        "    ax.text(0.95, 0.95, stats_text, transform=ax.transAxes, fontsize=10,\n",
        "           verticalalignment='top', horizontalalignment='right',\n",
        "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_xlabel(f'{reducer} Value')\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.set_title(f'Permutation Distribution for {reducer}')\n",
        "\n",
        "    ax.legend()\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_all_permutation_distributions(results: Dict, figsize: Tuple[int, int] = (12, 8)) -> Dict[str, Figure]:\n",
        "    figures = {}\n",
        "    for reducer_name, reducer_results in results.items():\n",
        "        # Convert reducer_name to string for safe dict key\n",
        "        key = str(reducer_name)\n",
        "        if (isinstance(reducer_results, dict) and\n",
        "            'permutation_distribution' in reducer_results and\n",
        "            'value' in reducer_results):\n",
        "            try:\n",
        "                fig = plot_permutation_distribution(results, reducer_name, figsize=figsize)\n",
        "                figures[key] = fig\n",
        "            except Exception as e:\n",
        "                print(f\"Error plotting {reducer_name}: {e}\")\n",
        "    return figures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rkp_nRhPysCt",
        "outputId": "68ee021c-47c1-4acc-ca95-e1e4575ef805"
      },
      "outputs": [],
      "source": [
        "normalize = 'z-score'\n",
        "random_seed = 42\n",
        "df = pd.read_csv(\"unemployment_rate_by_age_groups.csv\")\n",
        "\n",
        "orig_calc_table = pyspi_calc(df, \"test_config_yml\",yaml_str_stat_reduce, normalize=normalize)\n",
        "\n",
        "\n",
        "print(permutation_test(\n",
        "    df,\n",
        "    \"test_config_yml\",\n",
        "    yaml_str_stat_reduce,\n",
        "    orig_calc_table,\n",
        "    n_permutations=5,\n",
        "    normalize=normalize,\n",
        "    random_seed=random_seed,\n",
        "    save_permutation_data=True,\n",
        "    orig_calc_table=orig_calc_table\n",
        "))\n",
        "\n",
        "permutation_results = permutation_test(\n",
        "    df,\n",
        "    \"test_config_yml\",\n",
        "    yaml_str_stat_reduce,\n",
        "    orig_calc_table,\n",
        "    n_permutations=5,\n",
        "    normalize=normalize,\n",
        "    random_seed=random_seed,\n",
        "    save_permutation_data=True,\n",
        "    orig_calc_table=orig_calc_table\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vY7iQYI6PUc9"
      },
      "outputs": [],
      "source": [
        "def bootstrap_sample_data(df: pd.DataFrame, sample_fraction: float = 0.9, random_seed: Optional[int] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a bootstrap sample from the dataframe by sampling a fraction of rows with replacement.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        The original dataframe\n",
        "    sample_fraction : float, optional\n",
        "        Fraction of rows to sample\n",
        "    random_seed : Optional[int], optional\n",
        "        Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    pd.DataFrame\n",
        "        Bootstrap sampled dataframe (rows)\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        np.random.seed(random_seed)\n",
        "\n",
        "    # Calculate number of rows to sample\n",
        "    n_samples = int(len(df) * sample_fraction)\n",
        "\n",
        "    # Sample row indices with replacement\n",
        "    sampled_indices = np.random.choice(df.index, size=n_samples, replace=True)\n",
        "\n",
        "    # Create bootstrap sample with selected rows\n",
        "    bootstrap_df = df.loc[sampled_indices].copy()\n",
        "    return bootstrap_df\n",
        "\n",
        "\n",
        "def pyspi_calc_bootstrap(df: pd.DataFrame,config_name: str, yaml, normalize: Optional[str] = None,\n",
        "                        sample_fraction: float = 0.9, random_seed: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Calculate covariance and other statistics from a CSV file using bootstrap sampling.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df : pd.DataFrame\n",
        "        The DataFrame containing the data to change\n",
        "    config_name: str\n",
        "        Name of the configuration file\n",
        "    yaml : YAML string for configuration\n",
        "    normalize : Optional[str], optional\n",
        "        Method to normalize data (default: None)\n",
        "    sample_fraction : float, optional\n",
        "        Fraction of rows to sample for bootstrap (default: 0.9)\n",
        "    random_seed : Optional[int], optional\n",
        "        Seed for reproducible sampling (default: None)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Dictionary of calculated statistics\n",
        "    \"\"\"\n",
        "    # Load data \n",
        "    df = load_data(df)\n",
        "\n",
        "    # Create bootstrap sample\n",
        "    bootstrap_df = bootstrap_sample_data(df, sample_fraction=sample_fraction, random_seed=random_seed)\n",
        "    if normalize is not None:\n",
        "        bootstrap_df = normalize_data(bootstrap_df, method=normalize)\n",
        "    # Use Calculator class to compute the statistics\n",
        "    cfg = Config.from_yaml(config_name, yaml)\n",
        "    calc = Calculator(bootstrap_df, normalise=False)\n",
        "    calc.compute(cfg)\n",
        "    return calc.results\n",
        "\n",
        "\n",
        "def bootstrap_test(\n",
        "    df: pd.DataFrame,\n",
        "    config_name,\n",
        "    yaml,\n",
        "    calc,\n",
        "    n_bootstrap: int = 1000,\n",
        "    sample_fraction: float = 0.9,\n",
        "    normalize: Optional[str] = None,\n",
        "    random_seed: Optional[int] = None,\n",
        "    save_bootstrap_data: bool = True,\n",
        "    orig_calc_table: Optional[Dict] = None,  \n",
        ") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Perform bootstrap sampling test to assess variability of statistics.\n",
        "    \"\"\"\n",
        "    REDUCERS = list(calc.columns.values)\n",
        "    # Use provided orig_calc_table or compute if not given\n",
        "    if orig_calc_table is None:\n",
        "        orig_calc_table = pyspi_calc(df, config_name,yaml, normalize=normalize)\n",
        "    \n",
        "    orig_results = {\n",
        "        reducer: orig_calc_table[reducer].values[0] for reducer in REDUCERS\n",
        "    }\n",
        "    \n",
        "    # Store bootstrap values for variance calculation and plotting\n",
        "    bootstrap_values = {reducer: [] for reducer in REDUCERS}\n",
        "\n",
        "    # Run bootstrap samples with progress bar\n",
        "    iter_range = tqdm(range(n_bootstrap), desc=\"Bootstrap sampling\")\n",
        "    for i in iter_range:\n",
        "        # Use different random seed for each bootstrap sample\n",
        "        bootstrap_seed = None if random_seed is None else random_seed + i + 1\n",
        "        # Get bootstrap sample statistics\n",
        "        bootstrap_calc_table = pyspi_calc_bootstrap(df, config_name,yaml, normalize=normalize,\n",
        "                                                   sample_fraction=sample_fraction, random_seed=bootstrap_seed)\n",
        "\n",
        "        for reducer_name in REDUCERS:\n",
        "            try:\n",
        "                bootstrap_value = bootstrap_calc_table[reducer_name].values[0]\n",
        "                bootstrap_values[reducer_name].append(bootstrap_value)\n",
        "            except:\n",
        "                continue\n",
        "            \n",
        "    # Calculate bootstrap statistics and add to results\n",
        "    final_results = {}\n",
        "    for reducer_name in REDUCERS:\n",
        "        orig_value = orig_results[reducer_name]\n",
        "        result = {\n",
        "            'original_value': orig_value,\n",
        "            'n_bootstrap': n_bootstrap,\n",
        "            'sample_fraction': sample_fraction\n",
        "        }\n",
        "\n",
        "        if save_bootstrap_data and bootstrap_values[reducer_name]:\n",
        "            bootstrap_data = bootstrap_values[reducer_name]\n",
        "\n",
        "            # Calculate confidence intervals (2.5% and 97.5% percentiles for 95% CI)\n",
        "            ci_lower = np.percentile(bootstrap_data, 2.5)\n",
        "            ci_upper = np.percentile(bootstrap_data, 97.5)\n",
        "\n",
        "            result.update({\n",
        "                'bootstrap_mean': np.mean(bootstrap_data),\n",
        "                'bootstrap_median': np.median(bootstrap_data),\n",
        "                'bootstrap_std': np.std(bootstrap_data),\n",
        "                'bootstrap_variance': np.var(bootstrap_data),\n",
        "                'bootstrap_std_err': np.std(bootstrap_data) / np.sqrt(len(bootstrap_data)),\n",
        "                'bootstrap_min': np.min(bootstrap_data),\n",
        "                'bootstrap_max': np.max(bootstrap_data),\n",
        "                'bootstrap_ci_lower': ci_lower,\n",
        "                'bootstrap_ci_upper': ci_upper,\n",
        "                'bootstrap_distribution': bootstrap_data if save_bootstrap_data else None,\n",
        "                'original_in_ci': ci_lower <= orig_value <= ci_upper,\n",
        "                'bias': np.mean(bootstrap_data) - orig_value,\n",
        "                'coefficient_of_variation': np.std(bootstrap_data) / np.abs(np.mean(bootstrap_data)) * 100 if np.mean(bootstrap_data) != 0 else np.nan\n",
        "            })\n",
        "\n",
        "        final_results[reducer_name] = result\n",
        "\n",
        "    return final_results\n",
        "\n",
        "\n",
        "def plot_bootstrap_distribution(results: Dict, reducer,\n",
        "                               figsize: Tuple[int, int] = (10, 6)) -> Figure:\n",
        "    \"\"\"\n",
        "    Plot the distribution of bootstrap test values\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    results : Dict\n",
        "        Dictionary containing bootstrap test results.\n",
        "    reducer : str\n",
        "        Name of the reducer\n",
        "    figsize : Tuple[int, int], optional\n",
        "        Figure size (width, height), by default (10, 6)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Figure\n",
        "        Matplotlib figure object.\n",
        "    \"\"\"\n",
        "    # Extract data\n",
        "    result = results[reducer]\n",
        "    bootstrap_values = result['bootstrap_distribution']\n",
        "    orig_value = result['original_value']\n",
        "    ci_lower = result['bootstrap_ci_lower']\n",
        "    ci_upper = result['bootstrap_ci_upper']\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Plot histogram of bootstrap values\n",
        "    n, bins, patches = ax.hist(bootstrap_values, bins=30, alpha=0.7, color='lightgreen',\n",
        "                              edgecolor='black', density=True)\n",
        "\n",
        "    # Add kernel density\n",
        "    sns.kdeplot(bootstrap_values, color='darkgreen', ax=ax, linewidth=2)\n",
        "\n",
        "    # Add vertical line for original value\n",
        "    ax.axvline(x=orig_value, color='red', linestyle='--', linewidth=2,\n",
        "              label=f'Original value: {orig_value:.4f}')\n",
        "\n",
        "    # Add confidence interval lines\n",
        "    ax.axvline(x=ci_lower, color='orange', linestyle=':', linewidth=2,\n",
        "              label=f'95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]')\n",
        "    ax.axvline(x=ci_upper, color='orange', linestyle=':', linewidth=2)\n",
        "\n",
        "    # Fill confidence interval area\n",
        "    ax.axvspan(ci_lower, ci_upper, alpha=0.2, color='orange')\n",
        "\n",
        "    # Add bootstrap statistics\n",
        "    stats_text = (\n",
        "        f\"Bootstrap Stats:\\n\"\n",
        "        f\"Mean: {result['bootstrap_mean']:.4f}\\n\"\n",
        "        f\"Median: {result['bootstrap_median']:.4f}\\n\"\n",
        "        f\"Std Dev: {result['bootstrap_std']:.4f}\\n\"\n",
        "        f\"Std Err: {result['bootstrap_std_err']:.4f}\\n\"\n",
        "        f\"Bias: {result['bias']:.4f}\\n\"\n",
        "        f\"CV: {result['coefficient_of_variation']:.2f}%\\n\"\n",
        "        f\"Original in CI: {result['original_in_ci']}\"\n",
        "    )\n",
        "\n",
        "    # Add the stats text as an annotation\n",
        "    ax.text(0.95, 0.95, stats_text, transform=ax.transAxes, fontsize=10,\n",
        "           verticalalignment='top', horizontalalignment='right',\n",
        "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_xlabel(f'{reducer} Value')\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.set_title(f'Row Bootstrap Distribution for {reducer}')\n",
        "\n",
        "    # Add subtitle with sample info\n",
        "    subtitle = f'Row Bootstrap samples: {result[\"n_bootstrap\"]}, Sample fraction: {result[\"sample_fraction\"]:.1%}'\n",
        "    ax.text(0.5, 0.98, subtitle, transform=ax.transAxes, fontsize=9,\n",
        "           verticalalignment='top', horizontalalignment='center', style='italic')\n",
        "\n",
        "    ax.legend()\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_all_bootstrap_distributions(results: Dict, figsize: Tuple[int, int] = (12, 8)) -> Dict[str, Figure]:\n",
        "    figures = {}\n",
        "    for reducer_name, reducer_results in results.items():\n",
        "        key = str(reducer_name)\n",
        "        try:\n",
        "            fig = plot_bootstrap_distribution(results, reducer_name, figsize=figsize)\n",
        "            figures[key] = fig\n",
        "        except Exception as e:\n",
        "            print(f\"Error plotting bootstrap {reducer_name}: {e}\")\n",
        "    return figures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUHvjsalF8u6"
      },
      "outputs": [],
      "source": [
        "normalize = 'z-score'\n",
        "random_seed = 42\n",
        "df = pd.read_csv(\"unemployment_rate_by_age_groups.csv\")\n",
        "orig_calc_table = pyspi_calc(df, \"test_config_yml\",yaml_str_stat_reduce, normalize=normalize)\n",
        "\n",
        "print(bootstrap_test(\n",
        "    df,\n",
        "    \"test_config_yml\",\n",
        "    yaml_str_stat_reduce,\n",
        "    orig_calc_table,\n",
        "    n_bootstrap=3,\n",
        "    normalize=normalize,\n",
        "    random_seed=random_seed,\n",
        "    save_bootstrap_data=True,\n",
        "    orig_calc_table=orig_calc_table\n",
        "))\n",
        "\n",
        "bootstrap_results = bootstrap_test(\n",
        "    df,\n",
        "    \"test_config_yml\",\n",
        "    yaml_str_stat_reduce,\n",
        "    orig_calc_table,\n",
        "    n_bootstrap=3,\n",
        "    normalize=normalize,\n",
        "    random_seed=random_seed,\n",
        "    save_bootstrap_data=True,\n",
        "    orig_calc_table=orig_calc_table\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7VulBgCGO67"
      },
      "outputs": [],
      "source": [
        "plot_all_bootstrap_distributions(bootstrap_results)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def bootstrap_sample_columns(df: pd.DataFrame, sample_fraction: float = 0.9, random_seed: Optional[int] = None) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Create a bootstrap sample from the dataframe by sampling a fraction of columns without replacement.\n",
        "    \"\"\"\n",
        "    if random_seed is not None:\n",
        "        np.random.seed(random_seed)\n",
        "    n_samples = int(len(df.columns) * sample_fraction)\n",
        "    sampled_columns = np.random.choice(df.columns, size=n_samples, replace=False)\n",
        "    bootstrap_df = df.loc[:, sampled_columns].copy()\n",
        "    return bootstrap_df\n",
        "\n",
        "def pyspi_calc_bootstrap_columns(df: pd.DataFrame,config_name: str, yaml, normalize: Optional[str] = None,\n",
        "                        sample_fraction: float = 0.9, random_seed: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Calculate covariance and other statistics from a CSV file using bootstrap sampling.\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    df: pd.DataFrame\n",
        "        The DataFrame containing the data to change\n",
        "    config_name: str\n",
        "        Name of the configuration file\n",
        "    yaml : YAML string for configuration\n",
        "    normalize : Optional[str], optional\n",
        "        Method to normalize data (default: None)\n",
        "    sample_fraction : float, optional\n",
        "        Fraction of rows to sample for bootstrap (default: 0.9)\n",
        "    random_seed : Optional[int], optional\n",
        "        Seed for reproducible sampling (default: None)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Dictionary of calculated statistics\n",
        "    \"\"\"\n",
        "    # Load data with optional normalization\n",
        "    df = load_data(df)\n",
        "\n",
        "    # Create bootstrap sample\n",
        "    bootstrap_df = bootstrap_sample_columns(df, sample_fraction=sample_fraction, random_seed=random_seed)\n",
        "    if normalize is not None:\n",
        "        bootstrap_df = normalize_data(bootstrap_df, method=normalize)\n",
        "    # Use Calculator class to compute the statistics\n",
        "    cfg = Config.from_yaml(config_name, yaml)\n",
        "    calc = Calculator(bootstrap_df, normalise=False)\n",
        "    calc.compute(cfg)\n",
        "    return calc.results\n",
        "\n",
        "\n",
        "def bootstrap_test_columns(\n",
        "    df: pd.DataFrame,\n",
        "    config_name,\n",
        "    yaml,\n",
        "    calc,\n",
        "    n_bootstrap_columns: int = 1000,\n",
        "    sample_fraction_columns: float = 0.9,\n",
        "    normalize: Optional[str] = None,\n",
        "    random_seed: Optional[int] = None,\n",
        "    save_bootstrap_data: bool = True,\n",
        "    orig_calc_table: Optional[Dict] = None,  \n",
        ") -> Dict[str, Dict[str, Dict[str, Any]]]:\n",
        "    \"\"\"\n",
        "    Perform bootstrap sampling test to assess variability of statistics.\n",
        "    \"\"\"\n",
        "    REDUCERS = list(calc.columns.values)\n",
        "    # Use provided orig_calc_table or compute if not given\n",
        "    if orig_calc_table is None:\n",
        "        orig_calc_table = pyspi_calc(df, config_name,yaml, normalize=normalize)\n",
        "    \n",
        "    orig_results = {\n",
        "        reducer: orig_calc_table[reducer].values[0] for reducer in REDUCERS\n",
        "    }\n",
        "    \n",
        "    # Store bootstrap values for variance calculation and plotting\n",
        "    bootstrap_values = {reducer: [] for reducer in REDUCERS}\n",
        "\n",
        "    # Run bootstrap samples with progress bar\n",
        "    iter_range = tqdm(range(n_bootstrap_columns), desc=\"Bootstrap sampling\")\n",
        "    for i in iter_range:\n",
        "        # Use different random seed for each bootstrap sample\n",
        "        bootstrap_seed = None if random_seed is None else random_seed + i + 1\n",
        "        # Get bootstrap sample statistics\n",
        "        bootstrap_calc_table = pyspi_calc_bootstrap_columns(df, config_name,yaml, normalize=normalize,\n",
        "                                                   sample_fraction=sample_fraction_columns, random_seed=bootstrap_seed)\n",
        "\n",
        "        for reducer_name in REDUCERS:\n",
        "            try:\n",
        "                bootstrap_value = bootstrap_calc_table[reducer_name].values[0]\n",
        "                bootstrap_values[reducer_name].append(bootstrap_value)\n",
        "            except:\n",
        "                continue\n",
        "            \n",
        "    # Calculate bootstrap statistics and add to results\n",
        "    \n",
        "    final_results = {}\n",
        "    for reducer_name in REDUCERS:\n",
        "        orig_value = orig_results[reducer_name]\n",
        "        result = {\n",
        "            'original_value': orig_value,\n",
        "            'n_bootstrap': n_bootstrap_columns,\n",
        "            'sample_fraction': sample_fraction_columns\n",
        "        }\n",
        "\n",
        "        if save_bootstrap_data and bootstrap_values[reducer_name]:\n",
        "            bootstrap_data = bootstrap_values[reducer_name]\n",
        "\n",
        "            # Calculate confidence intervals (2.5% and 97.5% percentiles for 95% CI)\n",
        "            ci_lower = np.percentile(bootstrap_data, 2.5)\n",
        "            ci_upper = np.percentile(bootstrap_data, 97.5)\n",
        "\n",
        "            result.update({\n",
        "                'bootstrap_mean': np.mean(bootstrap_data),\n",
        "                'bootstrap_median': np.median(bootstrap_data),\n",
        "                'bootstrap_std': np.std(bootstrap_data),\n",
        "                'bootstrap_variance': np.var(bootstrap_data),\n",
        "                'bootstrap_std_err': np.std(bootstrap_data) / np.sqrt(len(bootstrap_data)),\n",
        "                'bootstrap_min': np.min(bootstrap_data),\n",
        "                'bootstrap_max': np.max(bootstrap_data),\n",
        "                'bootstrap_ci_lower': ci_lower,\n",
        "                'bootstrap_ci_upper': ci_upper,\n",
        "                'bootstrap_distribution': bootstrap_data if save_bootstrap_data else None,\n",
        "                'original_in_ci': ci_lower <= orig_value <= ci_upper,\n",
        "                'bias': np.mean(bootstrap_data) - orig_value,\n",
        "                'coefficient_of_variation': np.std(bootstrap_data) / np.abs(np.mean(bootstrap_data)) * 100 if np.mean(bootstrap_data) != 0 else np.nan\n",
        "            })\n",
        "\n",
        "        final_results[reducer_name] = result\n",
        "\n",
        "    return final_results\n",
        "\n",
        "def plot_bootstrap_distribution_columns(results: Dict, reducer,\n",
        "                               figsize: Tuple[int, int] = (10, 6)) -> Figure:\n",
        "    \"\"\"\n",
        "    Plot the distribution of bootstrap test values\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    results : Dict\n",
        "        Dictionary containing bootstrap test results.\n",
        "    reducer : str\n",
        "        Name of the reducer\n",
        "    figsize : Tuple[int, int], optional\n",
        "        Figure size (width, height), by default (10, 6)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Figure\n",
        "        Matplotlib figure object.\n",
        "    \"\"\"\n",
        "    # Extract data\n",
        "    result = results[reducer]\n",
        "    bootstrap_values = result['bootstrap_distribution']\n",
        "    orig_value = result['original_value']\n",
        "    ci_lower = result['bootstrap_ci_lower']\n",
        "    ci_upper = result['bootstrap_ci_upper']\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Plot histogram of bootstrap values\n",
        "    n, bins, patches = ax.hist(bootstrap_values, bins=30, alpha=0.7, color='lightgreen',\n",
        "                              edgecolor='black', density=True)\n",
        "\n",
        "    # Add kernel density\n",
        "    sns.kdeplot(bootstrap_values, color='darkgreen', ax=ax, linewidth=2)\n",
        "\n",
        "    # Add vertical line for original value\n",
        "    ax.axvline(x=orig_value, color='red', linestyle='--', linewidth=2,\n",
        "              label=f'Original value: {orig_value:.4f}')\n",
        "\n",
        "    # Add confidence interval lines\n",
        "    ax.axvline(x=ci_lower, color='orange', linestyle=':', linewidth=2,\n",
        "              label=f'95% CI: [{ci_lower:.4f}, {ci_upper:.4f}]')\n",
        "    ax.axvline(x=ci_upper, color='orange', linestyle=':', linewidth=2)\n",
        "\n",
        "    # Fill confidence interval area\n",
        "    ax.axvspan(ci_lower, ci_upper, alpha=0.2, color='orange')\n",
        "\n",
        "    # Add bootstrap statistics\n",
        "    stats_text = (\n",
        "        f\"Bootstrap Stats:\\n\"\n",
        "        f\"Mean: {result['bootstrap_mean']:.4f}\\n\"\n",
        "        f\"Median: {result['bootstrap_median']:.4f}\\n\"\n",
        "        f\"Std Dev: {result['bootstrap_std']:.4f}\\n\"\n",
        "        f\"Bias: {result['bias']:.4f}\\n\"\n",
        "        f\"Std Err: {result['bootstrap_std_err']:.4f}\\n\"\n",
        "        f\"CV: {result['coefficient_of_variation']:.2f}%\\n\"\n",
        "        f\"Original in CI: {result['original_in_ci']}\"\n",
        "    )\n",
        "\n",
        "    # Add the stats text as an annotation\n",
        "    ax.text(0.95, 0.95, stats_text, transform=ax.transAxes, fontsize=10,\n",
        "           verticalalignment='top', horizontalalignment='right',\n",
        "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_xlabel(f'{reducer} Value')\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.set_title(f'Column Bootstrap Distribution for {reducer}')\n",
        "\n",
        "    # Add subtitle with sample info\n",
        "    subtitle = f'Column Bootstrap samples: {result[\"n_bootstrap\"]}, Sample fraction: {result[\"sample_fraction\"]:.1%}'\n",
        "    ax.text(0.5, 0.98, subtitle, transform=ax.transAxes, fontsize=9,\n",
        "           verticalalignment='top', horizontalalignment='center', style='italic')\n",
        "\n",
        "    ax.legend()\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_all_bootstrap_distributions_columns(results: Dict, figsize: Tuple[int, int] = (12, 8)) -> Dict[str, Figure]:\n",
        "    figures = {}\n",
        "    for reducer_name, reducer_results in results.items():\n",
        "        key = str(reducer_name)\n",
        "        try:\n",
        "            fig = plot_bootstrap_distribution_columns(results, reducer_name, figsize=figsize)\n",
        "            figures[key] = fig\n",
        "        except Exception as e:\n",
        "            print(f\"Error plotting bootstrap {reducer_name}: {e}\")\n",
        "    return figures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perturbation_test(\n",
        "    df: pd.DataFrame,\n",
        "    config_name,\n",
        "    yaml,\n",
        "    calc,\n",
        "    n_perturbations: int = 1000,\n",
        "    scale_factor: float = 0.1,\n",
        "    normalize: Optional[str] = None,\n",
        "    random_seed: Optional[int] = None,\n",
        "    save_perturbation_data: bool = True,\n",
        "    orig_calc_table: Optional[Dict] = None,\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Perform a perturbation test to assess statistical significance and calculate variance.\n",
        "    \"\"\"\n",
        "    REDUCERS = list(calc.columns.values)\n",
        "    # Use provided orig_calc_table or compute\n",
        "    if orig_calc_table is None:\n",
        "        orig_calc_table = pyspi_calc(df, config_name, yaml, normalize=normalize)\n",
        "\n",
        "    orig_results = {\n",
        "        reducer: orig_calc_table[reducer].values[0] for reducer in REDUCERS\n",
        "    }\n",
        "    \n",
        "    perturb_values = {reducer: [] for reducer in REDUCERS}\n",
        "\n",
        "    # Run perturbations with progress bar\n",
        "    iter_range = tqdm(range(n_perturbations), desc=\"Perturbation sampling\")\n",
        "    for i in iter_range:\n",
        "        # Use different random seed for each perm\n",
        "        perturb_seed = None if random_seed is None else random_seed + i + 1\n",
        "        # Get shuffled data statistics\n",
        "        perturb_calc_table = pyspi_calc(df, config_name,yaml, normalize=normalize, perturb=True, scale_factor=scale_factor,random_seed=perturb_seed)\n",
        "\n",
        "        for reducer in REDUCERS:\n",
        "            perturb_value = perturb_calc_table[reducer].values[0]\n",
        "            # Append the value to the list\n",
        "            perturb_values[reducer].append(perturb_value)\n",
        "\n",
        "    # Calculate p-values, variances, and add to results\n",
        "    # Create the DataFrame of perturbation results AFTER collecting all values\n",
        "    # The index length should match the number of perturbations\n",
        "    perturb_data = pd.DataFrame(perturb_values)\n",
        "    # Calculate p-values, variances, and add to results\n",
        "    final_results = {}\n",
        "    for reducer in REDUCERS:\n",
        "        # Access the specific reducer's data from the perm_data DataFrame\n",
        "        perturb_data_series = perturb_data[reducer]\n",
        "        orig_value = orig_results[reducer]\n",
        "        n = len(perturb_data_series) # Use the length of the collected data\n",
        "\n",
        "        # Calculate permutation percentile for p-value\n",
        "        lower_tail = np.sum(perturb_data_series < orig_value) / n\n",
        "        upper_tail = np.sum(perturb_data_series > orig_value) / n\n",
        "        perturbation_percentile = min(lower_tail, upper_tail)\n",
        "        p_value = perturbation_percentile * 2 # Multiply by 2 for a two-tailed test\n",
        "\n",
        "        result = {'value': orig_value,\n",
        "              'p_value': p_value}\n",
        "\n",
        "        # Update results with permutation statistics\n",
        "        result.update({\n",
        "            'perturbation_mean': np.mean(perturb_data_series),\n",
        "            'perturbation_median': np.median(perturb_data_series),\n",
        "            'perturbation_std': np.std(perturb_data_series),\n",
        "            'perturbation_variance': np.var(perturb_data_series),\n",
        "            'perturbation_std_err': np.std(perturb_data_series) / np.sqrt(n),\n",
        "            'perturbation_min': np.min(perturb_data_series),\n",
        "            'perturbation_max': np.max(perturb_data_series),\n",
        "            'perturbation_distribution': perturb_data_series.tolist() if save_perturbation_data else None, # Store as list if needed\n",
        "            'perturbation_percentile_rank': perturbation_percentile\n",
        "        })\n",
        "\n",
        "        final_results[reducer] = result\n",
        "    return final_results\n",
        "\n",
        "\n",
        "def plot_perturbation_distribution(results: Dict, reducer: str,\n",
        "                                 figsize: Tuple[int, int] = (10, 6)) -> Figure:\n",
        "    \"\"\"\n",
        "    Plot the distribution of perturbation test values\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    results : Dict\n",
        "        Dictionary containing perturbation test results.\n",
        "    reducer : str\n",
        "        Name of the reducer\n",
        "    figsize : Tuple[int, int], optional\n",
        "        Figure size (width, height), by default (10, 6)\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    Figure\n",
        "        Matplotlib figure object.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract data\n",
        "    result = results[reducer]\n",
        "    perturb_values = result['perturbation_distribution']\n",
        "    orig_value = result['value']\n",
        "    p_value = result['p_value']\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Plot histogram of permutation values\n",
        "    n, bins, patches = ax.hist(perturb_values, bins=30, alpha=0.7, color='skyblue',\n",
        "                              edgecolor='black', density=True)\n",
        "\n",
        "    # Add kernel density estimate\n",
        "    sns.kdeplot(perturb_values, color='navy', ax=ax, linewidth=2)\n",
        "\n",
        "    # Add vertical line for original value\n",
        "    ax.axvline(x=orig_value, color='red', linestyle='--', linewidth=2,\n",
        "              label=f'Original value: {orig_value:.4f}')\n",
        "\n",
        "    # Add permutation statistics\n",
        "    stats_text = (\n",
        "        f\"Perturbation Stats:\\n\"\n",
        "        f\"Mean: {result['perturbation_mean']:.4f}\\n\"\n",
        "        f\"Median: {result['perturbation_median']:.4f}\\n\"\n",
        "        f\"Std Dev: {result['perturbation_std']:.4f}\\n\"\n",
        "        f\"Std Err: {result['perturbation_std_err']:.4f}\\n\"\n",
        "        f\"p-value: {p_value:.4f}\"\n",
        "    )\n",
        "\n",
        "    # Add the stats text as an annotation\n",
        "    ax.text(0.95, 0.95, stats_text, transform=ax.transAxes, fontsize=10,\n",
        "           verticalalignment='top', horizontalalignment='right',\n",
        "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_xlabel(f'{reducer} Value')\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.set_title(f'Perturbation Distribution for {reducer}')\n",
        "\n",
        "    ax.legend()\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_all_perturbation_distributions(results: Dict, figsize: Tuple[int, int] = (12, 8)) -> Dict[str, Figure]:\n",
        "    figures = {}\n",
        "    for reducer_name, reducer_results in results.items():\n",
        "        # Convert reducer_name to string for safe dict key\n",
        "        key = str(reducer_name)\n",
        "        if (isinstance(reducer_results, dict) and\n",
        "            'perturbation_distribution' in reducer_results and\n",
        "            'value' in reducer_results):\n",
        "            try:\n",
        "                fig = plot_perturbation_distribution(results, reducer_name, figsize=figsize)\n",
        "                figures[key] = fig\n",
        "            except Exception as e:\n",
        "                print(f\"Error plotting {reducer_name}: {e}\")\n",
        "    return figures\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def perturbation_test(\n",
        "    df: pd.DataFrame,\n",
        "    config_name,\n",
        "    yaml,\n",
        "    calc,\n",
        "    n_perturbations: int = 1000,\n",
        "    scale_factor: float = 0.1,\n",
        "    normalize: Optional[str] = None,\n",
        "    random_seed: Optional[int] = None,\n",
        "    save_perturbation_data: bool = True,\n",
        "    orig_calc_table: Optional[Dict] = None,\n",
        ") -> Dict[str, Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Perform a perturbation test to assess statistical significance and calculate variance.\n",
        "    Now also computes a 90% confidence interval for each reducer.\n",
        "    \"\"\"\n",
        "    REDUCERS = list(calc.columns.values)\n",
        "    # Use provided orig_calc_table or compute\n",
        "    if orig_calc_table is None:\n",
        "        orig_calc_table = pyspi_calc(df, config_name, yaml, normalize=normalize)\n",
        "\n",
        "    orig_results = {\n",
        "        reducer: orig_calc_table[reducer].values[0] for reducer in REDUCERS\n",
        "    }\n",
        "    \n",
        "    perturb_values = {reducer: [] for reducer in REDUCERS}\n",
        "\n",
        "    # Run perturbations with progress bar\n",
        "    iter_range = tqdm(range(n_perturbations), desc=\"Perturbation sampling\")\n",
        "    for i in iter_range:\n",
        "        # Use different random seed for each perm\n",
        "        perturb_seed = None if random_seed is None else random_seed + i + 1\n",
        "        # Get shuffled data statistics\n",
        "        perturb_calc_table = pyspi_calc(df, config_name,yaml, normalize=normalize, perturb=True, scale_factor=scale_factor,random_seed=perturb_seed)\n",
        "\n",
        "        for reducer in REDUCERS:\n",
        "            perturb_value = perturb_calc_table[reducer].values[0]\n",
        "            # Append the value to the list\n",
        "            perturb_values[reducer].append(perturb_value)\n",
        "\n",
        "    # Calculate p-values, variances, and add to results\n",
        "    # Create the DataFrame of perturbation results AFTER collecting all values\n",
        "    # The index length should match the number of perturbations\n",
        "    perturb_data = pd.DataFrame(perturb_values)\n",
        "    # Calculate p-values, variances, and add to results\n",
        "    final_results = {}\n",
        "    for reducer in REDUCERS:\n",
        "        # Access the specific reducer's data from the perm_data DataFrame\n",
        "        perturb_data_series = perturb_data[reducer]\n",
        "        orig_value = orig_results[reducer]\n",
        "        n = len(perturb_data_series) # Use the length of the collected data\n",
        "\n",
        "        # Calculate permutation percentile for p-value\n",
        "        lower_tail = np.sum(perturb_data_series < orig_value) / n\n",
        "        upper_tail = np.sum(perturb_data_series > orig_value) / n\n",
        "        perturbation_percentile = min(lower_tail, upper_tail)\n",
        "        p_value = perturbation_percentile * 2 # Multiply by 2 for a two-tailed test\n",
        "\n",
        "        # Calculate 90% confidence interval (5th and 95th percentiles)\n",
        "        ci_lower = np.percentile(perturb_data_series, 5)\n",
        "        ci_upper = np.percentile(perturb_data_series, 95)\n",
        "\n",
        "        result = {'value': orig_value,\n",
        "              'p_value': p_value}\n",
        "\n",
        "        # Update results with permutation statistics and CI\n",
        "        result.update({\n",
        "            'perturbation_mean': np.mean(perturb_data_series),\n",
        "            'perturbation_median': np.median(perturb_data_series),\n",
        "            'perturbation_std': np.std(perturb_data_series),\n",
        "            'perturbation_variance': np.var(perturb_data_series),\n",
        "            'perturbation_std_err': np.std(perturb_data_series) / np.sqrt(n),\n",
        "            'perturbation_min': np.min(perturb_data_series),\n",
        "            'perturbation_max': np.max(perturb_data_series),\n",
        "            'perturbation_distribution': perturb_data_series.tolist() if save_perturbation_data else None, # Store as list if needed\n",
        "            'perturbation_percentile_rank': perturbation_percentile,\n",
        "            'perturbation_ci_lower': ci_lower,\n",
        "            'perturbation_ci_upper': ci_upper,\n",
        "            'original_in_ci': ci_lower <= orig_value <= ci_upper\n",
        "        })\n",
        "\n",
        "        final_results[reducer] = result\n",
        "    return final_results\n",
        "\n",
        "\n",
        "def plot_perturbation_distribution(results: Dict, reducer: str,\n",
        "                                 figsize: Tuple[int, int] = (10, 6)) -> Figure:\n",
        "    \"\"\"\n",
        "    Plot the distribution of perturbation test values, including a 90% confidence interval.\n",
        "    \"\"\"\n",
        "    # Extract data\n",
        "    result = results[reducer]\n",
        "    perturb_values = result['perturbation_distribution']\n",
        "    orig_value = result['value']\n",
        "    p_value = result['p_value']\n",
        "    ci_lower = result.get('perturbation_ci_lower', None)\n",
        "    ci_upper = result.get('perturbation_ci_upper', None)\n",
        "\n",
        "    # Create figure\n",
        "    fig, ax = plt.subplots(figsize=figsize)\n",
        "\n",
        "    # Plot histogram of permutation values\n",
        "    n, bins, patches = ax.hist(perturb_values, bins=30, alpha=0.7, color='skyblue',\n",
        "                              edgecolor='black', density=True)\n",
        "\n",
        "    # Add kernel density estimate\n",
        "    sns.kdeplot(perturb_values, color='navy', ax=ax, linewidth=2)\n",
        "\n",
        "    # Add vertical line for original value\n",
        "    ax.axvline(x=orig_value, color='red', linestyle='--', linewidth=2,\n",
        "              label=f'Original value: {orig_value:.4f}')\n",
        "\n",
        "    # Add confidence interval lines if available\n",
        "    if ci_lower is not None and ci_upper is not None:\n",
        "        ax.axvline(x=ci_lower, color='orange', linestyle=':', linewidth=2,\n",
        "                  label=f'90% CI: [{ci_lower:.4f}, {ci_upper:.4f}]')\n",
        "        ax.axvline(x=ci_upper, color='orange', linestyle=':', linewidth=2)\n",
        "        ax.axvspan(ci_lower, ci_upper, alpha=0.2, color='orange')\n",
        "\n",
        "    # Add permutation statistics\n",
        "    stats_text = (\n",
        "        f\"Perturbation Stats:\\n\"\n",
        "        f\"Mean: {result['perturbation_mean']:.4f}\\n\"\n",
        "        f\"Median: {result['perturbation_median']:.4f}\\n\"\n",
        "        f\"Std Dev: {result['perturbation_std']:.4f}\\n\"\n",
        "        f\"Std Err: {result['perturbation_std_err']:.4f}\\n\"\n",
        "        f\"p-value: {p_value:.4f}\\n\"\n",
        "        f\"90% CI: [{ci_lower:.4f}, {ci_upper:.4f}]\\n\"\n",
        "        f\"Original in CI: {result.get('original_in_ci', False)}\"\n",
        "    )\n",
        "\n",
        "    # Add the stats text as an annotation\n",
        "    ax.text(0.95, 0.95, stats_text, transform=ax.transAxes, fontsize=10,\n",
        "           verticalalignment='top', horizontalalignment='right',\n",
        "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
        "\n",
        "    # Add labels and title\n",
        "    ax.set_xlabel(f'{reducer} Value')\n",
        "    ax.set_ylabel('Density')\n",
        "    ax.set_title(f'Perturbation Distribution for {reducer}')\n",
        "\n",
        "    ax.legend()\n",
        "    fig.tight_layout()\n",
        "\n",
        "    return fig\n",
        "\n",
        "\n",
        "def plot_all_perturbation_distributions(results: Dict, figsize: Tuple[int, int] = (12, 8)) -> Dict[str, Figure]:\n",
        "    figures = {}\n",
        "    for reducer_name, reducer_results in results.items():\n",
        "        # Convert reducer_name to string for safe dict key\n",
        "        key = str(reducer_name)\n",
        "        if (isinstance(reducer_results, dict) and\n",
        "            'perturbation_distribution' in reducer_results and\n",
        "            'value' in reducer_results):\n",
        "            try:\n",
        "                fig = plot_perturbation_distribution(results, reducer_name, figsize=figsize)\n",
        "                figures[key] = fig\n",
        "            except Exception as e:\n",
        "                print(f\"Error plotting {reducer_name}: {e}\")\n",
        "    return figures\n",
        "\n",
        "#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"unemployment_rate_by_age_groups.csv\")\n",
        "perturb = perturbation_test(\n",
        "    df,\n",
        "    \"test_config_yml\",\n",
        "    yaml_str_stat_reduce,\n",
        "    orig_calc_table,\n",
        "    n_perturbations=5,\n",
        "    normalize=normalize,\n",
        "    random_seed=random_seed,\n",
        "    save_perturbation_data=True,\n",
        "    orig_calc_table=orig_calc_table\n",
        ")\n",
        "plot_all_perturbation_distributions(perturb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GzNqFlbYK23K"
      },
      "outputs": [],
      "source": [
        "def run_permutation_analysis(\n",
        "    df: pd.DataFrame,\n",
        "    config_name,\n",
        "    yaml,\n",
        "    n_permutations: int = 1000,\n",
        "    n_bootstrap_row: int = 1000,\n",
        "    n_bootstrap_col: int = 1000,\n",
        "    n_perturbations: int = 1000,\n",
        "    scale_factor: float = 0.1,\n",
        "    bootstrap_sample_fraction_row: float = 0.9,\n",
        "    bootstrap_sample_fraction_col: float = 0.9,\n",
        "    normalize: Optional[str] = None,\n",
        "    random_seed: Optional[int] = None,\n",
        "    pdf_output: str = None\n",
        ") -> Dict:\n",
        "    \"\"\"\n",
        "    Run a comprehensive permutation and bootstrap analysis with combined visualizations and detailed summaries.\n",
        "    \"\"\"\n",
        "    print(f\"{n_permutations} permutations, {n_bootstrap_row} row bootstrap samples, {n_bootstrap_col} column bootstrap samples, {n_perturbations} perturbations\")\n",
        "\n",
        "    # Calculate orig_calc_table once\n",
        "    orig_calc_table = pyspi_calc(df, config_name,yaml, normalize=normalize)\n",
        "\n",
        "    # Permutation test\n",
        "    print(\"Permutation analysis:\")\n",
        "    permutation_results = permutation_test(\n",
        "        df,\n",
        "        config_name,\n",
        "        yaml,\n",
        "        orig_calc_table,\n",
        "        n_permutations=n_permutations,\n",
        "        normalize=normalize,\n",
        "        random_seed=random_seed,\n",
        "        save_permutation_data=True,\n",
        "        orig_calc_table=orig_calc_table,\n",
        "    )\n",
        "\n",
        "    # Row bootstrap test\n",
        "    print(\"Row bootstrap analysis:\")\n",
        "    bootstrap_results = bootstrap_test(\n",
        "        df,\n",
        "        config_name,\n",
        "        yaml,\n",
        "        orig_calc_table,\n",
        "        n_bootstrap=n_bootstrap_row,\n",
        "        sample_fraction=bootstrap_sample_fraction_row,\n",
        "        normalize=normalize,\n",
        "        random_seed=random_seed,\n",
        "        save_bootstrap_data=True,\n",
        "        orig_calc_table=orig_calc_table,\n",
        "    )\n",
        "\n",
        "    # # Column bootstrap test (leave-one-column-out)\n",
        "    # print(\"Column bootstrap analysis (jackknife):\")\n",
        "    # column_bootstrap_results_jackknife = bootstrap_test_columns_jackknife(\n",
        "    #     filename=filename,\n",
        "    #     statistic_names=statistic_names,\n",
        "    #     reducer_names=all_reducers,\n",
        "    #     normalize=normalize,\n",
        "    #     save_bootstrap_data=True,\n",
        "    #     orig_calc_table=orig_calc_table,\n",
        "    # )\n",
        "\n",
        "    # Column bootstrap test (random columns)\n",
        "    print(\"Column bootstrap analysis (random columns):\")\n",
        "    column_bootstrap_results_random = bootstrap_test_columns(\n",
        "        df,\n",
        "        config_name,\n",
        "        yaml,\n",
        "        orig_calc_table,\n",
        "        n_bootstrap_columns=n_bootstrap_col,\n",
        "        sample_fraction_columns=bootstrap_sample_fraction_col,\n",
        "        normalize=normalize,\n",
        "        random_seed=random_seed,\n",
        "        save_bootstrap_data=True,\n",
        "        orig_calc_table=orig_calc_table,\n",
        "    )\n",
        "\n",
        "\n",
        "    # Perturbation test\n",
        "    print(\"Perturbation analysis:\")\n",
        "    perturbation_results = perturbation_test(\n",
        "        df,\n",
        "        config_name,\n",
        "        yaml,\n",
        "        orig_calc_table,\n",
        "        n_perturbations=n_perturbations,\n",
        "        scale_factor=scale_factor,\n",
        "        normalize=normalize,\n",
        "        random_seed=random_seed,\n",
        "        save_perturbation_data=True,\n",
        "        orig_calc_table=orig_calc_table,\n",
        "    )\n",
        "\n",
        "    # Combine results\n",
        "    results = {\n",
        "        'permutation_results': permutation_results,\n",
        "        'row_bootstrap_results': bootstrap_results,\n",
        "        # 'column_bootstrap_results_jackknife': column_bootstrap_results_jackknife,\n",
        "        'column_bootstrap_results_random': column_bootstrap_results_random,\n",
        "        'perturbation_results': perturbation_results,\n",
        "        'metadata': {\n",
        "            'n_permutations': n_permutations,\n",
        "            'n_bootstrap_row': n_bootstrap_row,\n",
        "            'n_bootstrap_col': n_bootstrap_col,\n",
        "            'n_perturbations': n_perturbations,\n",
        "            'scale_factor': scale_factor,\n",
        "            'bootstrap_sample_fraction_row': bootstrap_sample_fraction_row,\n",
        "            'bootstrap_sample_fraction_col': bootstrap_sample_fraction_col,\n",
        "            'normalize': normalize,\n",
        "            'random_seed': random_seed\n",
        "        }\n",
        "    }\n",
        "\n",
        "    print(\"Generating permutation distribution plots...\")\n",
        "    permutation_distribution_plots = plot_all_permutation_distributions(permutation_results)\n",
        "    print(\"Generating row bootstrap distribution plots...\")\n",
        "    row_bootstrap_plots = plot_all_bootstrap_distributions(bootstrap_results)\n",
        "    # print(\"Generating column bootstrap (jackknife) distribution plots...\")\n",
        "    # column_bootstrap_plots = plot_all_column_bootstrap_distributions(column_bootstrap_results_jackknife, method=\"Jackknife\")\n",
        "    print(\"Generating column bootstrap (random) distribution plots...\")\n",
        "    column_bootstrap_random_plots = plot_all_bootstrap_distributions_columns(column_bootstrap_results_random)\n",
        "    print(\"Generating perturbation distribution plots...\")\n",
        "    perturbation_distribution_plots = plot_all_perturbation_distributions(perturbation_results)\n",
        "\n",
        "    results['permutation_distribution_plots'] = permutation_distribution_plots\n",
        "    results['row_bootstrap_plots'] = row_bootstrap_plots\n",
        "    # results['column_bootstrap_plots'] = column_bootstrap_plots\n",
        "    results['column_bootstrap_random_plots'] = column_bootstrap_random_plots\n",
        "    results['perturbation_distribution_plots'] = perturbation_distribution_plots\n",
        "    with PdfPages(pdf_output) as pdf:\n",
        "        # Add a metadata page\n",
        "        fig = plt.figure(figsize=(8, 10))\n",
        "        plt.axis('off')\n",
        "        metadata_text = (\n",
        "            f\"Number of permutations: {n_permutations}\\n\"\n",
        "            f\"Number of bootstrap samples: {n_bootstrap_row}\\n\"\n",
        "            f\"Row bootstrap sample fraction: {bootstrap_sample_fraction_row:.1%}\\n\"\n",
        "            f\"Column bootstrap sample fraction: {bootstrap_sample_fraction_row:.1%}\\n\"\n",
        "            f\"Number of perturbations: {n_perturbations}\\n\"\n",
        "            f\"Scale factor for perturbations: {scale_factor}\\n\"\n",
        "            f\"Normalization: {normalize or 'None'}\\n\"\n",
        "            f\"Random seed: {random_seed or 'None'}\\n\"\n",
        "        )\n",
        "\n",
        "        plt.text(0.5, 0.5, metadata_text, ha='center', va='center', fontsize=10, transform=fig.transFigure)\n",
        "        pdf.savefig(fig)\n",
        "        plt.close(fig)\n",
        "\n",
        "        # Save permutation distribution plots\n",
        "        for fig in results['permutation_distribution_plots'].values():\n",
        "            pdf.savefig(fig)\n",
        "            plt.close(fig)\n",
        "        # Save row bootstrap distribution plots\n",
        "        for fig in results['row_bootstrap_plots'].values():\n",
        "            pdf.savefig(fig)\n",
        "            plt.close(fig)\n",
        "\n",
        "        # # Save column bootstrap (jackknife) distribution plots\n",
        "        # for stat_name, reducer_plots in results['column_bootstrap_plots'].items():\n",
        "        #     for reducer_name, fig in reducer_plots.items():\n",
        "        #         pdf.savefig(fig)\n",
        "        #         plt.close(fig)\n",
        "        # Save column bootstrap (random) distribution plots\n",
        "        for fig in results['column_bootstrap_random_plots'].values():\n",
        "            pdf.savefig(fig)\n",
        "            plt.close(fig)\n",
        "        # Save perturbation distribution plots\n",
        "        for fig in results['perturbation_distribution_plots'].values():\n",
        "            pdf.savefig(fig)\n",
        "            plt.close(fig)\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtC3ueQiWKNc"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"unemployment_rate_by_age_groups.csv\")\n",
        "run_permutation_analysis(\n",
        "    df =df,\n",
        "    config_name=\"test_config_yml\",\n",
        "    yaml=yaml_str_stat_reduce,\n",
        "    n_permutations=10,\n",
        "    n_bootstrap_row=10,\n",
        "    n_bootstrap_col=10,\n",
        "    n_perturbations=10,\n",
        "    scale_factor=0.1,\n",
        "    bootstrap_sample_fraction_row=0.9,\n",
        "    bootstrap_sample_fraction_col=0.9,\n",
        "    random_seed=42,\n",
        "    pdf_output=\"combined_analysis.pdf\",\n",
        "    normalize=\"z-score\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run_permutation_analysis(\n",
        "    df=df,\n",
        "    config_name=\"test_config_yml\",\n",
        "    yaml=yaml_str_stat_reduce,\n",
        "    n_permutations=10,\n",
        "    n_bootstrap_row=10,\n",
        "    n_bootstrap_col=10,\n",
        "    n_perturbations=10,\n",
        "    scale_factor=0.1,\n",
        "    bootstrap_sample_fraction_row=0.9,\n",
        "    bootstrap_sample_fraction_col=0.9,\n",
        "    random_seed=42,\n",
        "    pdf_output=\"combined_analysis_daily_delhi.pdf\",\n",
        "    normalize=\"z-score\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def n_p_plane_analysis(\n",
        "        \n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Xj6GdO95Sxx"
      },
      "source": [
        "## notes and debugg-y things (safely ignore)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "faCRnhvra1So"
      },
      "outputs": [],
      "source": [
        "# cfg = Config.from_internal(\"fast\")\n",
        "# - this is breaking - note"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
